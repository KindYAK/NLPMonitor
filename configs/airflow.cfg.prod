[core]
dags_folder=/opt/bitnami/airflow/dags
base_log_folder=/opt/bitnami/airflow/logs
remote_logging=False
remote_log_conn_id=
remote_base_log_folder=
encrypt_s3_logs=False
logging_level=INFO
fab_logging_level=WARN
logging_config_class=
colored_console_log=True
colored_log_format=[%%(blue)s%%(asctime)s%%(reset)s] {%%(blue)s%%(filename)s:%%(reset)s%%(lineno)d} %%(log_color)s%%(levelname)s%%(reset)s - %%(log_color)s%%(message)s%%(reset)s
colored_formatter_class=airflow.utils.log.colored_log.CustomTTYColoredFormatter
log_format=[%%(asctime)s] {%%(filename)s:%%(lineno)d} %%(levelname)s - %%(message)s
simple_log_format=%%(asctime)s %%(levelname)s - %%(message)s
log_filename_template={{ ti.dag_id }}/{{ ti.task_id }}/{{ ts }}/{{ try_number }}.log
log_processor_filename_template={{ filename }}.log
dag_processor_manager_log_location=/opt/bitnami/airflow/logs/dag_processor_manager/dag_processor_manager.log
hostname_callable=socket:getfqdn
default_timezone=utc
executor=CeleryExecutor
sql_engine_encoding=utf-8
sql_alchemy_pool_enabled=True
sql_alchemy_pool_size=5000
sql_alchemy_max_overflow=50
sql_alchemy_pool_recycle=1800
sql_alchemy_reconnect_timeout=300
sql_alchemy_schema=
parallelism=100
dag_concurrency=15
non_pooled_task_slot_count=0
dags_are_paused_at_creation=True
max_active_runs_per_dag=3
load_examples=False
plugins_folder=/opt/bitnami/airflow/plugins
fernet_key=LD2nT24yYyLD2okKJgbIJmiv-IsxkSkj0Rt7BO92mj4=
donot_pickle=False
dagbag_import_timeout=30
task_runner=StandardTaskRunner
default_impersonation=
security=
secure_mode=False
unit_test_mode=False
task_log_reader=task
enable_xcom_pickling=True
killed_task_cleanup_time=60
dag_run_conf_overrides_params=False
worker_precheck=False
dag_discovery_safe_mode=True

[cli]
api_client=airflow.api.client.local_client
endpoint_url=http://{hostname}:8001/airflow

[api]
auth_backend=airflow.api.auth.backend.default

[lineage]
backend=

[atlas]
sasl_enabled=False
host=
port=21000
username=
password=

[operators]
default_owner=airflow
default_cpus=4
default_ram=30720
default_disk=30720
default_gpus=1

[hive]
default_hive_mapred_queue=

[webserver]
base_url=http://{hostname}:8001/airflow
web_server_host=0.0.0.0
web_server_port=8001
web_server_ssl_cert=
web_server_ssl_key=
web_server_master_timeout=120
web_server_worker_timeout=120
worker_refresh_batch_size=4
worker_refresh_interval=30
secret_key=temporary_key
workers=4
worker_class=sync
access_logfile=-
error_logfile=-
expose_config=False
authenticate=False
filter_by_owner=False
owner_mode=user
dag_default_view=tree
dag_orientation=LR
demo_mode=False
log_fetch_timeout_sec=5
hide_paused_dags_by_default=False
page_size=100
rbac=True
navbar_color=
default_dag_run_display_number=25
enable_proxy_fix=False
cookie_secure=False
cookie_samesite=
default_wrap=False

[email]
email_backend=airflow.utils.email.send_email_smtp

[smtp]
smtp_host=localhost
smtp_starttls=True
smtp_ssl=False
smtp_port=25
smtp_mail_from=airflow@example.com

[celery]
celery_app_name=airflow.executors.celery_executor
worker_concurrency=1
worker_log_server_port=8793
broker_url=redis://:@airflow-redis:6379/1
flower_host=0.0.0.0
flower_url_prefix=
flower_port=5555
flower_basic_auth=
default_queue=default
sync_parallelism=2
celery_config_options=airflow.config_templates.default_celery.DEFAULT_CELERY_CONFIG
ssl_active=False
ssl_key=
ssl_cert=
ssl_cacert=
pool=prefork

[dask]
cluster_address=127.0.0.1:8786
tls_ca=
tls_cert=
tls_key=

[scheduler]
job_heartbeat_sec=15
scheduler_heartbeat_sec=15
run_duration=-1
min_file_process_interval=0
dag_dir_list_interval=300
print_stats_interval=60
scheduler_health_check_threshold=30
child_process_log_directory=/opt/bitnami/airflow/logs/scheduler
scheduler_zombie_task_threshold=300
catchup_by_default=True
max_tis_per_query=512
statsd_on=False
statsd_host=localhost
statsd_port=8125
statsd_prefix=airflow
max_threads=2
authenticate=False
use_job_schedule=True

[ldap]
uri=
user_filter=objectClass=*
user_name_attr=uid
group_member_attr=memberOf
superuser_filter=
data_profiler_filter=
bind_user=cn=Manager,dc=example,dc=com
bind_password=insecure
basedn=dc=example,dc=com
cacert=/etc/ca/ldap_ca.crt
search_scope=LEVEL
ignore_malformed_schema=False

[mesos]
master=localhost:5050
framework_name=Airflow
task_cpu=1
task_memory=256
checkpoint=False
authenticate=False

[kerberos]
ccache=/tmp/airflow_krb5_ccache
principal=airflow
reinit_frequency=3600
kinit_path=kinit
keytab=airflow.keytab

[github_enterprise]
api_rev=v3

[admin]
hide_sensitive_variable_fields=True

[elasticsearch]
host=
log_id_template={dag_id}-{task_id}-{execution_date}-{try_number}
end_of_log_mark=end_of_log
frontend=
write_stdout=False
json_format=False
json_fields=asctime, filename, lineno, levelname, message

[kubernetes]
worker_container_repository=
worker_container_tag=
worker_container_image_pull_policy=IfNotPresent
delete_worker_pods=True
worker_pods_creation_batch_size=1
namespace=default
airflow_configmap=
dags_in_image=False
dags_volume_subpath=
dags_volume_claim=
logs_volume_subpath=
logs_volume_claim=
dags_volume_host=
logs_volume_host=
env_from_configmap_ref=
env_from_secret_ref=
git_repo=
git_branch=
git_subpath=
git_user=
git_password=
git_sync_root=/git
git_sync_dest=repo
git_dags_folder_mount_point=
git_ssh_key_secret_name=
git_ssh_known_hosts_configmap_name=
git_sync_container_repository=k8s.gcr.io/git-sync
git_sync_container_tag=v3.1.1
git_sync_init_container_name=git-sync-clone
worker_service_account_name=
image_pull_secrets=
gcp_service_account_keys=
in_cluster=True
affinity=
tolerations=
kube_client_request_args=
run_as_user=
fs_group=
